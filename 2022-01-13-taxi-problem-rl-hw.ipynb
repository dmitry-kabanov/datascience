{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0fd6807",
   "metadata": {},
   "source": [
    "# 04 Homework ðŸ‹ï¸ðŸ‹ï¸ðŸ‹ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf6613",
   "metadata": {},
   "source": [
    "#### ðŸ‘‰A course without homework is not a course!\n",
    "\n",
    "#### ðŸ‘‰Spend some time thinking and trying to implement the challenges I propose here.\n",
    "\n",
    "#### ðŸ‘‰They are not so easy, so if you get stuck drop me an email at `plabartabajo@gmail.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f82e45",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67656662",
   "metadata": {},
   "source": [
    "## 1. Can you update the function `train` in a way that the input `epsilon` can also be a callable function?\n",
    "\n",
    "An `epsilon` value that decays after each episode works better than a fixed `epsilon` for most RL problems.\n",
    "\n",
    "This is hard exercise, but I want you to give it a try.\n",
    "\n",
    "If you do not manage it, do not worry. We are going to implement this in an upcoming lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2580a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Callable, List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5745a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "\n",
    "    def __init__(self, env, alpha, gamma):\n",
    "        self.env = env\n",
    "\n",
    "        # table with q-values: n_states * n_actions\n",
    "        self.q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "        # hyper-parameters\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\"\"\"\n",
    "        # stop()\n",
    "        return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update_parameters(self, state, action, reward, next_state):\n",
    "        \"\"\"\"\"\"\n",
    "        old_value = self.q_table[state, action]\n",
    "        next_max = np.max(self.q_table[next_state])\n",
    "\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "        self.q_table[state, action] = new_value\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Sets q-values to zeros, which essentially means the agent does not know\n",
    "        anything\n",
    "        \"\"\"\n",
    "        self.q_table = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5cebf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_variable_epsilon(\n",
    "    agent,\n",
    "    env,\n",
    "    n_episodes: int,\n",
    "    epsilon: Union[float, callable]\n",
    ") -> Tuple[Any, List, List]:\n",
    "    \"\"\"\n",
    "    Trains and agent and returns 3 things:\n",
    "    - agent object\n",
    "    - timesteps_per_episode\n",
    "    - penalties_per_episode\n",
    "    \"\"\"\n",
    "    # For plotting metrics\n",
    "    timesteps_per_episode = []\n",
    "    penalties_per_episode = []\n",
    "\n",
    "    for i in tqdm(range(0, n_episodes)):\n",
    "\n",
    "        state = env.reset()\n",
    "\n",
    "        epochs, penalties, reward, = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if callable(epsilon):\n",
    "                eps = epsilon(i)\n",
    "            else:\n",
    "                eps = epsilon\n",
    "\n",
    "            if random.uniform(0, 1) < eps:\n",
    "                # Explore action space\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Exploit learned values\n",
    "                action = agent.get_action(state)\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            agent.update_parameters(state, action, reward, next_state)\n",
    "\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "\n",
    "            state = next_state\n",
    "            epochs += 1\n",
    "\n",
    "        timesteps_per_episode.append(epochs)\n",
    "        penalties_per_episode.append(penalties)\n",
    "\n",
    "    return agent, timesteps_per_episode, penalties_per_episode\n",
    "\n",
    "def schedule_epsilon(n_episode: int):\n",
    "    if n_episode > 50:\n",
    "        return 0.05\n",
    "    else:\n",
    "        return 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f5ebb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 126.13it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\").env\n",
    "alpha, gamma = 0.1, 0.9\n",
    "agent = QAgent(env, alpha, gamma)\n",
    "\n",
    "agent, timesteps, penalties = train_with_variable_epsilon(\n",
    "    agent, env, 100, schedule_epsilon\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e016e",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a46bf7",
   "metadata": {},
   "source": [
    "## 2. Can you parallelize the function `train_many_runs` using Python's `multiprocessing` module?\n",
    "\n",
    "I do not like to wait and stare at each progress bar, while I think that each run in `train_many_runs` could execute\n",
    "in parallel.\n",
    "\n",
    "Create a new function called `train_many_runs_in_parallel` that outputs the same results as `train_many_runs` but that executes in a fraction of time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
