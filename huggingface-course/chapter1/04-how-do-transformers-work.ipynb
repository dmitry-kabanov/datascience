{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05-how-do-transformers-work.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPoQ6qKeUGACgpSjexC1j/Z"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "XJUzV7v71YUA",
        "outputId": "e18a1ccd-00f6-4b8f-9de9-e7ec9e70e82c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"font-weight: bold; font-size: 36px\">\n",
              "    How do Transformers work?\n",
              "</div>\n",
              "\n",
              "<p>\n",
              "Source: <a href=\"https://huggingface.co/course/chapter1/4\">https://huggingface.co/course/chapter1/4</a>\n",
              "</p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%html\n",
        "<div style=\"font-weight: bold; font-size: 36px\">\n",
        "    How do Transformers work?\n",
        "</div>\n",
        "\n",
        "<p>\n",
        "Source: <a href=\"https://huggingface.co/course/chapter1/4\">https://huggingface.co/course/chapter1/4</a>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "Q9lyCPeK3ONo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer** is a neural network architecture that was introducted in the paper *Attention is all you need* in June 2017. The two most influential models are\n",
        "- GPT (and GPT-2, -3)\n",
        "- BERT and derivatives (DistilBERT, etc.)\n",
        "\n",
        "Broadly, Transformer models can be groupd like:\n",
        "- GPT-like (*auto-regressive* models)\n",
        "- BERT-like (*auto-encoding* models)\n",
        "- BART/T5-like (*sequence-to-sequence* models)"
      ],
      "metadata": {
        "id": "d0LuePIf13ZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers are language models"
      ],
      "metadata": {
        "id": "2ajtyoYf2x4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers are trained in self-supervised models (this means that humans do not have to label data in advance) using large amounts of raw text.\n",
        "\n",
        "In this way, the model develops general understanding of the language used for training but it is not good for a specific task. For that, a general pretrained model goes through *transfer learning*, in which the model is fine-tuned in a supervised way (with human-annotated datasets) for the task.\n",
        "\n",
        "Examples of tasks:\n",
        "\n",
        "- *Causal language modeling* - knowing the previous words, predict what word comes next\n",
        "- *Masked language modeling* - predict a masked word in a sentence"
      ],
      "metadata": {
        "id": "vQb0a-oM3L-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers are big models"
      ],
      "metadata": {
        "id": "4iQGnr3P4N_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bad side of transformers is that they are very large models and training and using them creates a lot of CO2 emissions.\n",
        "\n",
        "This means that transfer learning is very important. Hence, it helps the environment to take a pretrained language model and fine-tune it for a given task."
      ],
      "metadata": {
        "id": "Vk4O_7cO4Qe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer learning"
      ],
      "metadata": {
        "id": "bT8BRQkQ5DRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Pretraining* is the act of training a model from scratch: the weights are randomly initialized, and the model starts from this random state.\n",
        "\n",
        "It also takes a very large corpus of text and long training time.\n",
        "\n",
        "*Fine-tuning* is the training of a pretrained model. It is much faster and requires less data."
      ],
      "metadata": {
        "id": "YbtzoxY_5dT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General architecture\n",
        "\n",
        "The transformer architecture consists of two blocks:\n",
        "- encoder (takes an input and builds an internal representation of this input)\n",
        "- decoder (takes the internal representation along with other inputs and generate the output.\n",
        "\n",
        "These two blocks can be used independently:\n",
        "- encoder-only model: good for tasks as such named entity recognition where\n",
        "one needs good understanding of the input\n",
        "- decoder-only models: generate text\n",
        "- encoder-decoder models (aka seq-to-seq): for translation or summarization"
      ],
      "metadata": {
        "id": "jqc12hQH8Hwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention layers\n",
        "\n",
        "This is a key feature of Transformer models.\n",
        "Attention layers tell the model to pay special attention to some words in a sequence.\n",
        "This is often required, as a word can change meaning depending on the context.\n",
        "For example, when translating from English to German, \"my brother\" and \"my sister\" are translated differently as in Germany there is a a gender distinction: \"mein Bruder\" but \"meine Schwester\"."
      ],
      "metadata": {
        "id": "oU2QAz9y8weF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The original architecture\n",
        "\n",
        "Originally, Transformers were developed for translation tasks.\n",
        "During training, Encoder receives as input a text in a \"from\" language,\n",
        "while Decoder receives as input the same text but in the \"to\" language.\n",
        "\n",
        "In the encoder, the attention layers can use the whole sequence but in the decoder, only previously translated words can be used to predict, which word should follow.\n",
        "\n",
        "In the original architecture, the first attention layer in the decoer is given\n",
        "previously translated words, but the second attention layer has access to the encoder output, that is, it has access to all input words.\n",
        "\n",
        "The *attention mask* can be used to ignore certain words, for example, padding."
      ],
      "metadata": {
        "id": "QNLAlqX-9Dl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture vs. checkpoints\n",
        "\n",
        "* **Architecture:** This is the sceleton of the model - the definition of each layer and each operation\n",
        "* **Checkpoints:** these are concrete instances of the architecture, with given\n",
        "values of the model parameters\n",
        "* **Model:** This is a more ambigious term as it can mean both above terms depending on the context\n",
        "\n",
        "Example: BERT is an architecture but `bert-base-cased` that you can load using `from_pretrained` method, is a checkpoint, as it is a concrete model with given model parameters, obtained by the Google team for the first release of BERT.\n",
        "However, in general speak, people say \"the BERT model\" and \"the bert-base-cased model\"."
      ],
      "metadata": {
        "id": "ntcqNPYzqEDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tOOvPllnrBME"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}